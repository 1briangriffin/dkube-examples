{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "DATASET_SAMPLES  = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_NAME = ps_config['MONITOR_NAME']\n",
    "PRECOMPUTED_DATA_SOURCE = ps_config['PRECOMPUTED_DATA_SOURCE']\n",
    "PRESCORE_DATASET = ps_config[\"PRESCORE_DATASET\"]\n",
    "DKUBEUSERNAME = ps_config['DKUBEUSERNAME']\n",
    "TOKEN = ps_config['TOKEN']\n",
    "DKUBE_URL = ps_config['DKUBE_URL']\n",
    "ACCESS_KEY = ps_config['ACCESS_KEY']\n",
    "SECRET_KEY = ps_config['SECRET_KEY']\n",
    "DBHOSTNAME = ps_config['DBHOSTNAME']\n",
    "DATABASENAME = ps_config['DATABASENAME']\n",
    "DBUSERNAME = ps_config['DBUSERNAME']\n",
    "DBPASSWORD = ps_config['DBPASSWORD']\n",
    "DB_PROVIDER = ps_config['DB_PROVIDER']\n",
    "RUN_FREQUENCY = ps_config['RUN_FREQUENCY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.getenv(\"AWS_ACCESS_KEY_ID\") or ACCESS_KEY != os.getenv(\"AWS_ACCESS_KEY_ID\"):\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = ACCESS_KEY\n",
    "if not os.getenv(\"AWS_SECRET_ACCESS_KEY\") or SECRET_KEY != os.getenv(\"AWS_SECRET_ACCESS_KEY\"):\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = SECRET_KEY\n",
    "if not os.getenv(\"DBHOSTNAME\") or DBHOSTNAME != os.getenv(\"DBHOSTNAME\"):\n",
    "    os.environ[\"DBHOSTNAME\"]=DBHOSTNAME\n",
    "if not os.getenv(\"DBUSERNAME\") or DBUSERNAME != os.getenv(\"DBUSERNAME\"):\n",
    "    os.environ[\"DBUSERNAME\"]=DBUSERNAME\n",
    "if not os.getenv(\"DATABASENAME\") or DATABASENAME != os.getenv(\"DATABASENAME\"):\n",
    "    os.environ[\"DATABASENAME\"]=DATABASENAME\n",
    "if not os.getenv(\"DBPASSWORD\") or DBPASSWORD != os.getenv(\"DBPASSWORD\"):\n",
    "    os.environ[\"DBPASSWORD\"]=DBPASSWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.getenv(\"HOME\")\n",
    "if HOME:\n",
    "    EXECUTABLE_DIR = os.path.join(HOME,\".local\", \"bin\")\n",
    "    PATH = os.getenv(\"PATH\")\n",
    "    if EXECUTABLE_DIR not in PATH:\n",
    "        os.environ[\"PATH\"] = f\"{PATH}:{EXECUTABLE_DIR}\"\n",
    "    PATH = os.getenv(\"PATH\")\n",
    "if not os.getenv(\"AWS_BUCKET\"):\n",
    "    os.environ[\"AWS_BUCKET\"] = \"mm-workflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DB_PROVIDER == \"mssql\":\n",
    "    !sudo curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n",
    "    !curl https://packages.microsoft.com/config/ubuntu/$(cat /etc/lsb-release | grep DISTRIB_RELEASE | cut -f2 -d'=')/prod.list > /tmp/mssql-release.list\n",
    "    !sudo cp /tmp/mssql-release.list /etc/apt/sources.list.d/\n",
    "    !sudo apt-get update -y\n",
    "    !sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17\n",
    "    !sudo apt-get install unixodbc-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pymysql Flask-SQLAlchemy boto3 --user > /dev/null\n",
    "if DB_PROVIDER == \"mssql\":\n",
    "    !{sys.executable} -m pip install pyodbc --user\n",
    "elif DB_PROVIDER == \"mysql\":\n",
    "    !{sys.executable} -m pip install pymysql --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOME:\n",
    "    USR_LOCAL_LIB_PATH = os.path.join(HOME,\".local\",\"lib\",\"python3.6\",\"site-packages\")\n",
    "    if USR_LOCAL_LIB_PATH not in sys.path:\n",
    "        sys.path.append(USR_LOCAL_LIB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "from dkube.sdk.api import DkubeApi\n",
    "\n",
    "## Dependencies for data generator \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing as skpreprocessing\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from joblib import load\n",
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBCONFIG:\n",
    "    def __init__(self, hostname, databasename, username, password, provider=\"mysql\"):\n",
    "        self.hostname = hostname\n",
    "        self.databasename = databasename\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        if provider not in [\"mysql\", \"mssql\"]:\n",
    "            raise ValueError(f\"{provider} not a supported provider\")\n",
    "        self.provider = provider\n",
    "    \n",
    "    def __str__(self):\n",
    "        host_split = self.hostname.split(\":\")\n",
    "        if len(host_split) == 2:\n",
    "            if self.provider == \"mysql\":\n",
    "                return f\"mysql+pymysql://{self.username}:{self.password}@{host_split[0]}:{host_split[1]}/{self.databasename}\"\n",
    "            elif self.provider == \"mssql\":\n",
    "                params =  quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "                                    f\"SERVER={','.join(host_split)};\"\n",
    "                                    f\"DATABASE={self.databasename};\"\n",
    "                                    f\"UID={self.username};\"\n",
    "                                    f\"PWD={self.password}\")\n",
    "                return f\"mssql+pyodbc:///?odbc_connect={params}\"\n",
    "            else:\n",
    "                return \"\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class DataSource(Enum):\n",
    "    LOCAL = \"local\"\n",
    "    AWS_S3 = \"aws_s3\"\n",
    "    SQL = \"sql\"\n",
    "    \n",
    "\n",
    "DatasetSource = namedtuple('DatasetSource', 'model_monitor table frequency_unit data_class add_prefix_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENCY = f\"{RUN_FREQUENCY}m\"\n",
    "\n",
    "MODEL_FREQUENCY = RUN_FREQUENCY\n",
    "\n",
    "LABELLED_DATASET_TABLE  = \"insurance_precomputed\"\n",
    "\n",
    "LABELLED_DATA_CLASS = \"groundtruth\" #used for s3\n",
    "\n",
    "PREFIX_LABELLED_DATASET_WITH_TS = False\n",
    "\n",
    "## By default data source is local, supported are [DataSource.AWS_S3 and DataSource.SQL]\n",
    "\n",
    "if PRECOMPUTED_DATA_SOURCE == 'local':\n",
    "    DATASET_SOURCE = DataSource.LOCAL\n",
    "if PRECOMPUTED_DATA_SOURCE == 'aws-s3':\n",
    "    DATASET_SOURCE = DataSource.AWS_S3\n",
    "if PRECOMPUTED_DATA_SOURCE =='sql':\n",
    "    DATASET_SOURCE = DataSource.SQL\n",
    "    DBHOSTNAME = os.getenv(\"DBHOSTNAME\")\n",
    "    DATABASE_NAME = os.getenv(\"DATABASENAME\")\n",
    "    DBUSERNAME = os.getenv(\"DBUSERNAME\")\n",
    "    PASSWORD = os.getenv(\"DBPASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceDataGenerator:\n",
    "    # With no parameters or configuration, boto3 will look for\n",
    "    # access keys in these places:\n",
    "    # 1. Environment variables (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)\n",
    "    # 2. Credentials file (~/.aws/credentials or\n",
    "    #      C:\\Users\\USER_NAME\\.aws\\credentials)\n",
    "    # 3. AWS IAM role for Amazon EC2 instance\n",
    "    #    (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)\n",
    "\n",
    "    #    Define a ~/.aws/credentials file as following\n",
    "    #    [default]\n",
    "    #    aws_access_key_id=foo\n",
    "    #    aws_secret_access_key=bar\n",
    "    #    aws_session_token=baz # might not be required\n",
    "    BUCKET = None\n",
    "    S3_CLIENT = None\n",
    "    DB_ENGINE = None\n",
    "    API_CLIENT = None\n",
    "    TOKEN = None\n",
    "    USERNAME = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_time: datetime.datetime = None,\n",
    "        frequency=\"1H\",\n",
    "        model_frequency=10,\n",
    "        duration: str = \"10:24:12\",\n",
    "        margin=20,\n",
    "        db_config:DBCONFIG = None,\n",
    "        dataset_source: DataSource = DataSource.AWS_S3\n",
    "    ):\n",
    "        if not re.fullmatch(\"^\\d+[hmHM]{1}$\",frequency):\n",
    "            raise ValueError(\"frequency can have [time_value_int][time_unit] time_unit can be case case insensitive out of H, M\")\n",
    "        self.frequency  = frequency\n",
    "        self.margin=margin\n",
    "        self.monitor_name = MODEL_FREQUENCY\n",
    "        self.dataset_source = dataset_source\n",
    "        self.start_time = start_time if start_time else datetime.datetime.utcnow()\n",
    "        self.model_frequency = model_frequency\n",
    "        self.db_config = db_config\n",
    "            \n",
    "        self.duration = duration\n",
    "        klass = type(self)\n",
    "        if not klass.BUCKET:\n",
    "            klass.BUCKET = os.getenv(\"AWS_BUCKET\")\n",
    "        if not klass.S3_CLIENT:\n",
    "            klass.S3_CLIENT = boto3.client(\"s3\")\n",
    "        if not klass.TOKEN:\n",
    "            klass.TOKEN = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\",TOKEN)\n",
    "        if not klass.USERNAME:\n",
    "            klass.USERNAME= DKUBEUSERNAME\n",
    "        if not klass.API_CLIENT:\n",
    "            klass.API_CLIENT = DkubeApi(URL=os.getenv('DKUBE_URL',DKUBE_URL),token=klass.TOKEN)\n",
    "        if not klass.DB_ENGINE:\n",
    "            if str(self.db_config):\n",
    "                klass.DB_ENGINE = create_engine(str(self.db_config))\n",
    "\n",
    "        duration = self.duration.split(\"-\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "    \n",
    "    @classmethod\n",
    "    def save_dataset_to_s3(cls, data, monitor_name, name, typeofdata, prefix_dir_with_ts = True, frequency_unit=\"H\",current_date=None):\n",
    "        file_name = name + \".csv\"\n",
    "        print(filename)\n",
    "        if not current_date:\n",
    "            current_date = datetime.datetime.now()\n",
    "        data_dir = os.path.join(\n",
    "            monitor_name,\n",
    "            typeofdata\n",
    "        )\n",
    "        if prefix_dir_with_ts:\n",
    "            data_dir = os.path.join(data_dir, \n",
    "            current_date.strftime(\"%Y\"),\n",
    "            current_date.strftime(\"%m\"),\n",
    "            current_date.strftime(\"%d\"),\n",
    "            current_date.strftime(\"%H\"))\n",
    "            if frequency_unit.lower() ==\"m\":\n",
    "                data_dir = os.path.join(data_dir,current_date.strftime(\"%M\"))\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = cls.S3_CLIENT.put_object(\n",
    "                Bucket=cls.BUCKET, Key=file_path, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "      \n",
    "    @classmethod\n",
    "    def save_dataset_to_local(cls,data, name, monitor_name,typeofdata,frequency_unit=\"H\", current_date=None):\n",
    "        file_name = name + \".csv\"\n",
    "        try:\n",
    "            data_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "        except:\n",
    "            data_dir = os.getcwd()\n",
    "          \n",
    "        if not current_date:\n",
    "            current_date = datetime.datetime.now()\n",
    "        groundtruth_destination_path = os.path.join(HOME, \"dataset\", PRESCORE_DATASET,\n",
    "                                                   cls.API_CLIENT.get_dataset_versions(cls.USERNAME,PRESCORE_DATASET)[0]['version']['uuid'],\n",
    "                                                   \"data\")\n",
    "#         groundtruth_destination_path = HOME+'/dataset/'+PRESCORE_DATASET+cls.API_CLIENT.get_dataset_versions(cls.USERNAME,PRESCORE_DATASET)[0]['version']['uuid']+'/data/'\n",
    "        print(groundtruth_destination_path)\n",
    "        data_dir = groundtruth_destination_path\n",
    "        \n",
    "        if not os.path.isdir(data_dir):\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "        file_path = data_dir+'/'+file_name\n",
    "        data.to_csv(file_path, index=False)\n",
    "        \n",
    "        return file_path\n",
    "    \n",
    "    @classmethod\n",
    "    def save_dataset_to_sql(cls, data, tablename):\n",
    "        data.to_sql(tablename, cls.DB_ENGINE, if_exists=\"append\", index=False)\n",
    "    \n",
    "    def save_dataset(self ,data, data_name:str, config: DatasetSource, current_date=None):\n",
    "        klass = type(self)\n",
    "        if self.dataset_source == DataSource.AWS_S3:\n",
    "            return klass.save_dataset_to_s3(data, config.model_monitor, data_name, config.data_class, config.add_prefix_ts, config.frequency_unit, current_date)\n",
    "        elif self.dataset_source == DataSource.SQL:\n",
    "            klass.save_dataset_to_sql(data, config.table)\n",
    "        elif self.dataset_source == DataSource.LOCAL:\n",
    "            return klass.save_dataset_to_local(data, data_name, config.model_monitor, config.data_class, config.frequency_unit,current_date)\n",
    "\n",
    "    @property\n",
    "    def frequency_ts(self):\n",
    "        value = int(self.frequency[:-1])\n",
    "        unit = self.frequency[-1].lower()\n",
    "        seconds_per_unit = {\"s\": 1, \"m\": 60, \"h\": 3600, \"d\": 86400, \"w\": 604800}\n",
    "        seconds_count = int(value) * seconds_per_unit[unit]\n",
    "        now = datetime.datetime.utcnow()\n",
    "        if unit.lower() == \"h\":\n",
    "            delta = datetime.timedelta(hours=value)\n",
    "            new_time = (now+delta).replace(minute = 0, second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "            second_remaining = (new_time-now).seconds\n",
    "            result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "            print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "            return result        \n",
    "        elif unit == \"m\":\n",
    "            diff = abs(now.minute%-value)\n",
    "            if diff == 0:\n",
    "                delta = datetime.timedelta(minutes=value)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                result = (new_time-now).seconds\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "            else:\n",
    "                delta = datetime.timedelta(minutes = diff)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                second_remaining = (new_time-now).seconds\n",
    "                result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "        \n",
    "    @property\n",
    "    def awsS3Secret(self):\n",
    "        if PRECOMPUTED_DATA_SOURCE == 'aws_s3':\n",
    "            AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\",ACCESS_KEY) \n",
    "            AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\",SECRET_KEY)\n",
    "            print(AWS_ACCESS_KEY)\n",
    "        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "        else:\n",
    "            home_dir = os.getenv(\"HOME\")\n",
    "            if home_dir:\n",
    "                creds_path = os.path.join(home_dir, \".aws\",\"credentials\")\n",
    "                config = ConfigParser()\n",
    "                if os.path.isfile(creds_path):\n",
    "                    config.read(creds_path)\n",
    "                    if \"default\" in config:\n",
    "                        AWS_ACCESS_KEY = config[\"default\"][\"aws_access_key_id\"]\n",
    "                        AWS_SECRET_KEY = config[\"default\"][\"aws_secret_access_key\"]\n",
    "                        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "                            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "                \n",
    "    @property\n",
    "    def end(self):\n",
    "        duration = self.duration.split(\":\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "        return self.start_time + datetime.timedelta(\n",
    "            hours=int(duration[0]), minutes=int(duration[1]), seconds=int(duration[2])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InsuranceDataGenerator.URL = DKUBE_URL\n",
    "InsuranceDataGenerator.TOKEN = TOKEN\n",
    "InsuranceDataGenerator.API_CLIENT = DkubeApi(URL=DKUBE_URL, token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = InsuranceDataGenerator(MONITOR_NAME,\n",
    "                                   frequency=FREQUENCY,\n",
    "                                   model_frequency = MODEL_FREQUENCY,\n",
    "                                   db_config = DBCONFIG(\n",
    "                                       hostname=DBHOSTNAME,\n",
    "                                       databasename = DATABASENAME,\n",
    "                                       username = DBUSERNAME,\n",
    "                                       password = DBPASSWORD,\n",
    "                                       provider= DB_PROVIDER),\n",
    "                                   dataset_source = DATASET_SOURCE)\n",
    "\n",
    "ground_dataset_source = DatasetSource(model_monitor=MONITOR_NAME,\n",
    "                                      table=LABELLED_DATASET_TABLE,\n",
    "                                      data_class=LABELLED_DATA_CLASS,\n",
    "                                      frequency_unit = generator.frequency[-1],\n",
    "                                      add_prefix_ts=PREFIX_LABELLED_DATASET_WITH_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_precomputed_scores(no_of_scores=3):\n",
    "    df = {}\n",
    "    start = datetime.datetime.utcnow()\n",
    "    end = start + datetime.timedelta(seconds=10)\n",
    "    df[\"timestamp\"] = pd.date_range(start, end, no_of_scores)\n",
    "    df[\"accuracy\"] = np.random.uniform(low=0.8, high=0.95, size=no_of_scores)\n",
    "    df[\"precision\"] = np.random.uniform(low=0.8, high=0.95, size=no_of_scores)\n",
    "    df[\"recall\"] = np.random.uniform(low=0.8, high=0.95, size=no_of_scores)\n",
    "    df[\"roc_auc_score\"] = np.random.uniform(low=0.8, high=0.95, size=no_of_scores)\n",
    "    df[\"samples\"] = np.random.randint(low=80, high=100, size=no_of_scores)\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4])\n",
    "push_count = 1\n",
    "drift_path = []\n",
    "predict_path = []\n",
    "groundtruth_path = []\n",
    "for i in range(DATASET_SAMPLES):\n",
    "    second_remaining = generator.frequency_ts\n",
    "#     time.sleep(second_remaining)\n",
    "    print(\"Generating precomputed scores\")\n",
    "    precomputed_score = generate_precomputed_scores()\n",
    "    filename = f\"precomputed_score_{i+1}\"\n",
    "    g_path = generator.save_dataset(precomputed_score, filename,ground_dataset_source)\n",
    "    if g_path:\n",
    "        groundtruth_path.append(g_path)\n",
    "    print(f\"Pushed data for {ordinal(push_count)} time, Remaining pushes: {DATASET_SAMPLES-push_count}, Monitor name: {MONITOR_NAME}\")\n",
    "    push_count += 1\n",
    "print(\"***************** DATA GENERATION COMPLETED ******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
