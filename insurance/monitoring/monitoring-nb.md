# Create Monitor for Insurance Prediction Example Automatically

 This section explains how to use a JupyterLab .ipynb file to create a Monitor for the insurance prediction example.

 - This example uses the DKube built-in MinIO server and uses prediction datasets as "CloudEvents"
 - This example assumes that the serving cluster and the model monitoring cluster are the same

## Example Flow
 - Create the DKube resources.
 - Train a model for Insurance example using Tensorflow and deploy the model for inference
 - Create a Modelmonitor
   - There is a mandatory requirement to deploy a model for this example
 - Generate data for analysis by Modelmonitor
   - Predict data: Inference inputs/outputs
   - Label data:  Dataset with Groundtruth values for the inferences made above
  **In production, Label data would be generated by experts in the domain manually**
 - Cleanup resources after the example is complete

## Create Code & Model Repos

 > **_Note:_** This step may have been completed in an earlier section of this example.  If so, skip the steps here and use the Code & Model repo names that you previously created.  If you need to create a new Code and/or Model repo, follow the instructions at:
 - [Create Code Repo](../readme.md#create-project)
 - [Create Model Repo](../readme.md#create-model-repo)

## Create & Launch JupyterLab IDE

 > **_Note:_** This step may have been completed in an earlier section of this example.  If so, skip the steps here and use the JupyterLab IDE that you previously created.  If you need to create a new IDE, follow the instructions at:
 - [Create JupyterLab IDE](../readme.md#create-jupyterlab-ide)

 - Once the IDE is in the "Running" state, select the JupyterLab icon on the far right of the IDE line
   - This will create a JupyterLab tab

## Execute File to Create Monitor Resources

 - Navigate to "/workspace/\<your-code-repo\>/insurance/monitoring"
 - Open "resources.ipynb"
 - Fill in the external IP address in the field "SERVING_DKUBE_URL" in the form "https://\<IP Address\>:32222/
 - Leave the other fields in their current selection
 
4. Open Jupyterlab and from **workspace/insurance/insurance/monitoring** open **resources.ipynb** and fill the following details in the first cell.
    - **SERVING_DKUBE_URL** = {DKube url of serving cluster}
    - **SERVING_DKUBE_USERNAME** = {DKube username of serving cluster}
    - **SERVING_DKUBE_TOKEN** = {DKube authentication token of serving cluster}
    - if there is a sperate monitoring cluster then also fill the below details, otherwise leave these value empty.
      - **MONITORING_DKUBE_USERNAME** = {DKube username of monitoring cluster}
      - **MONITORING_DKUBE_TOKEN** = {DKube authentication token of monitoring cluster}
      - **MONITORING_DKUBE_URL** = {DKube URL of monitoring cluster}
    - **MONITOR_NAME** = {model monitor name}
    - Cloudevents are stored in DKube Minio bucket. Enter the following details from the **Serving DKube Cluster**
      - **MINIO_KEY** = {MINIO access key of Dkube setup where the prediction deployment is running. See below}
      - **MINIO_SECRET_KEY** = {MINIO access secret key of Dkube setup where the prediction deployment is running. See below}
      - MINIO_KEY and MINIO_SECRET_KEY values will be filled automatically by the example with SDK call, these values can also be obtained by running the following commands on the DKube setup where the prediction deployment is running. Provide the creds manually if the user is neither PE nor Operator on the remote cluster.
        - DKube API. Fill in DKUBE_IP and TOKEN in the following curl command
          - `curl -X 'GET' \
              'https://DKUBE_IP:32222/dkube/v2/controller/v2/deployments/logstore' \
              -H 'accept: application/json' \
              -H 'Authorization: Bearer <TOKEN>'`
        - If you have access to Kubernetes, you can get the secrets by running the following commands
          - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_ACCESS_KEY_ID}" | base64 -d`
          - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_SECRET_ACCESS_KEY}" | base64 -d`
    - The following will be derived from the environment automatically if the notebook is running inside same Dkube IDE. Otherwise in case if the notebook is running locally or in other Dkube Setup , then please fill in, 
5. Run all the cells. This will create all the dkube resources required for this example automatically. In case of seperate serving and monitoring cluster, the required resources will be created on the respective cluster.
6. Once all the cells complete the run you will see the following resources will get created,
   1. `insurance-data` dataset on both serving and monitoring cluster.
   2. `insurance-mm-kf-s3` dataset on monitoring cluster.


## Section 2: Insurance Model Training (Required to deploy model)

#### Note: This uses DKube Runs, Kubeflow Pipelines and KfServing. It requires full Dkube installed. 

1. From **workspace/insurance/insurance/** open **insurance_pipeline.ipynb** to build the pipeline.
2. The pipeline includes preprocessing, training and serving stages. Run all cells
     - **training**: the training stage takes the generated dataset as input, train a sgd model and outputs the model.
     - **serving**: The serving stage takes the generated model and serve it with a predict endpoint for inference.
3. Verify that the pipeline has created the following resources
     - Model: 'insurance' with an additional version.

## Section 3: Modelmonitoring
DKube provides Python SDK for creating a modelmonitor programmatically. You could also choose to create a modelmonitor from the DKube UI. This example cuurently support creating monitor using SDK. 

1. From **workspace/insurance/insurance/monitoring** open **modelmonitor.ipynb** and run all the cells. New model monitor will be created. If monitoring cluster details are given, it will also add the serving cluster in the monitoring cluster, and import the deployment on monitoring cluster. 
2. Predict and Groundtruth dataset data will be generated by Data Generation step and will be utilised by modelmonitor.
3. After the completion of the notebook, you will see the model monitor `insurance-mm-kf` in active state.

## Section 4: Data Generation
1. Open [data_generation.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/data_generation.ipynb) notebook for making predictions with the deployemnt endpoint and generate groundtruth datasets.
2. In 1st cell, Update Frequency according to what you set in Modelmonitor for Drift. For eg: for 5 minutes, specify it as `5m` and to specify the frequency in hours use `5h` for 5 hours interval.
3. Then Run All Cells. It will start Pushing the data. It uses the data definitions specified in resources.ipynb file.

**Note:** Livedata will be created on the MINIO under deployment id. In the case of minimal DKube, we will create on the serving cluster minio where deployments are running.

## Section 5: SMTP Settings (Optional)
Configure your SMTP server settings on Operator screen. This is optional. If SMTP server is not configured, no email alerts will be generated.

## Section 6: Cleanup
1. After your experiment is complete, 
   - Open **modelmonitor.ipynb** and set CLEANUP=True in last Cleanup cell and run.
   - Open **resources.ipynb** and set CLEANUP=True in last Cleanup cell and run.

